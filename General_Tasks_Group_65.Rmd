---
title: "General tasks Group 65"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results= 'hide')
```

### Summersemester 2020

### 1.

Import of the needed packages

```{r, message=FALSE}
if(!require("install.load")){
  install.packages("install.load")
}
library('install.load')

install_load("stringr", "data.table", "tidyverse","plotly","fitdistrplus")
```


### Preparation of the data for the exercise:
1.Import of the datasets: Komponente_K7.csv, Logistics_delay_K7.csv

The head () function is used to explore the imported datasets. When exploring the dataset, we noticed that the values in the column "Herstellernummer" are not matching the corresponding number in the ID for 112 in the dataset "Delivery_K7". But since this is irrelevant for the further analysis, the dataset error was not corrected.

Why irrelevant?? or both datasets??

```{r, results='markup'}
Production_K7 <- fread(file="./Data/Logistikverzug/Komponente_K7.csv") %>%
  as_tibble() %>%
  mutate(Produktionsdatum=as.Date(Produktionsdatum))%>%
  rename(production_date=Produktionsdatum) %>%
  rename(IDNumber=IDNummer)
head(Production_K7)

Delivery_K7 <- fread(file="./Data/Logistikverzug/Logistics_delay_K7.csv") %>%
  as_tibble() %>%
  mutate(Wareneingang=as.Date(Wareneingang)) %>%
  rename(good_arrrival=Wareneingang) %>%
  rename(IDNumber=IDNummer)
head(Delivery_K7)
```

2. Filter for relevant columns for the analysis

```{r}
Production_K7 <-Production_K7[,c("IDNumber","production_date")]

Delivery_K7 <-Delivery_K7[,c("IDNumber","good_arrrival")]

```

3.Join both datasets by ID number

We join the two datasets by a left join by the column "IDNumber, because we only want to add the row "good_arrival" to the matching IDNumber. Afterwards we again inspect the new Dataframe with the head function to check if the join worked.

```{r,results='markup'}
Prod_Deli_join <- Production_K7 %>%
  left_join(Delivery_K7,by="IDNumber")
head(Prod_Deli_join)
```

4. Add another colomn with difference between delivery and production date

First we calculate a vector "difference" with the difftime() function, which creates a time intervals between the columns good_arrival and production_date in days. Then this vector is added with the mutate call to the joined dataframe and saved as the final dataset: Logistics_delay
```{r,results='markup'}
#calculate difference between delivery and produciton
difference <- as.numeric(difftime(Prod_Deli_join$good_arrrival, Prod_Deli_join$production_date, units = "days"))

# add difference colomn to joined dataset 
Logistics_delay <- Prod_Deli_join %>%
  mutate(difference=difference)
head(Logistics_delay)
```

### 1.a)*How is the logistics delay distributed? Proof your selection by statistical tests and briefly describe your approach.*
To determine the distrubtion of the logistics delay it is necessary to fit one or more distributions to the data set and then evaluate the goodness of fit of these distributions.
So first, to find good distrubtion candidates for the fitting to the data set it is a good idea to observe the emperical distrubtion with different plots. Therefore the plotdist() function from the package fitdistrplus is used, which provides two plots: the left plot is the histogram on a density scale and the right plot the empirical cumulative distribution function (CDF)

```{r,results='markup'}
#plot histogram and cdf 
plotdist(difference, histo = T, demp = T)

```

Another good indicator to choose the distrubtion candidates is the cullen and frey grap also called pearson graph by calling the function descdist from the package fitdistrplus. This graph gives a first impression, which distrubtions could fit the data of the logicitcs delay good. 
In this plot it can be observed that Normal, Negativ Binomial and Poission seem to be good candidates because the blue observation point is near to their points or lines in the graph.

```{r,results='markup'}
#gain overview with cullen and frey graph
descdist(difference, discrete = T)
```

Resulting from this selection of distrubtion candidates we fit the dataset to these three distrubtions by the fitdist() function. Additionally to gain a bigger knowledge we plot ..

From the plot we can already see that the normal seems to fit the logistics delay the best.


```{r,results='markup'}
#fit different distrubtions to the data 
fit_nb  <- fitdist(difference, "nbinom")
fit_n  <- fitdist(difference, "norm")
fit_p <- fitdist(difference, "pois")

#plot distrubtions
par(mfrow = c(2, 2))
plot.legend <- c("N-Binomial", "Normal", "Poisson")

denscomp(list(fit_nb, fit_n, fit_p), legendtext = plot.legend,xlab="Logistics in days")
qqcomp(list(fit_nb, fit_n, fit_p), legendtext = plot.legend,xlab="Logistics in days")
cdfcomp(list(fit_nb, fit_n, fit_p), legendtext = plot.legend,xlab="Logistics in days")
ppcomp(list(fit_nb, fit_n, fit_p), legendtext = plot.legend,xlab="Logistics in days")
```


Added to our visual impression we calcuate the goodness of fit by calling the function gofstat(), which provides us with .... From the statitics we can see that the normal distrubtion provides the best fit to the data set.

```{r,results='markup'}
#calcuate goodness of fit of the three distrubutions
gofstat(list(fit_nb, fit_n, fit_p))
```


### 1.b)*What is the minimum/maximum time between delivering and receiving goods?*

### 1.c)*Determine the mean of the logistics delay.*

We determine the mean and min and max of the logistics delay by using the function summary. In the Results we see that the mean is 10.08 days, the max is 18 days and the min is 7 days.

```{r, results='markup'}
#determine  mean, max and min of logistics delay
summary(Logistics_delay$difference)
```

### 1.d)*Visualize the distribution in an appropriate way by displaying a histogram and the density function using the package plotly.*

```{r, results='markup'}
#hist1 <- plot_ly(x=Logistics_delay$difference, type = "histogram", histnorm = "probability")
#hist1

#p <- ggplot(Logistics_delay, aes(difference)) + 
#  geom_histogram(aes(y = ..density..), alpha = 0.7, fill = #"#333333") + 
#  geom_density(fill = "#ff4d4d", alpha = 0.5) + 
#  theme(panel.background = element_rect(fill = '#ffffff')) + 
#  ggtitle("Density with Histogram overlay")

#fig <- ggplotly(p)

#fig
#create a ggplot qq graph
#qq_plot <- ggplot(Logistics_delay, aes(sample=difference)) +
  #geom_qq()

#convert qq_plot in plotly format
#ggplotly(qq_plot)
```



### 2.*Why does it make sense to store the available data in separate files instead of saving everything in a huge table? Name at least four benefits. The available tables represent a typical data base structure. How is it called?*


This data base structure is called relational data base. The benefits from this structure, compared to one huge table is that the data is stored just once, which  eliminates data deduplication and therefor offers a higher accuracy. Furthermore the access is higher, because complex queries are easy for users to perform instead of navigating through a huge table. Another benefit is that relational databases are so flexible that your  can change the structure e.g. add table, rename relation and still keep the database running and queries happen. Last but not least, the tables of the relational database can be tagged as confidential or not. This segregation is easier implemented than with a huge table and therefor increases the security.

### 3.*How many of the parts T4 ended up in vehicles registered in the city of Dortmund?*

Our first step in this exercise is to create a iterable list of the paths to the files which contains a "Bestandteile" in the name. So we have the parts tables for each component. 

```{r, results='markup'}
#create list of file correspoding to the parts for the components
components_parts <- list.files("./Data/Komponente", full.names = T, pattern = "Bestandteile")
head(components_parts)
```

Next we define a function which first reads in the in the first step found relevant files. Then the function checks with a if clause, if at least on column names of the dataframe contains a "T4" at the end. This to determine whether the part 4, which has the column name "ID_T4" is used for this components or not. If this if clause is true only the columns for the ID of T4 and the correspoing are selected. 

```{r, results='markup'}
#create function to find the components which use part 4
task_3_1 <- function(x){
  df <- fread(file = x, header = T) #read file 
  if (any(str_detect(colnames(df),"T4$")==TRUE)){ #check if any column of the read dataframe has a name with T4 at the end 
    df %>%
      dplyr::select(ID_T4, contains("K")) # select ID_T$ column and columns, which contains K in the name 
  }
}
```

next the function is called with the lapply() function to do the same for all files found in the first step and then bind the rows together for all the tables created. So we have a table with a column for each component which has a part 4 in their parts list.

```{r, results='markup'}
#apply function task_3_1 to all components list and bind rows together
components_part4 <- bind_rows(lapply(components_parts,task_3_1))
head(components_part4)
```

In the results we can see that only for compoments K1BE1 the part 4 is used, because this the only column apart from ID_T4


To find the ID of the vehicles we know from the shortcuts table by the chair that only OEM 1 is using the compoment K1BE1 as their engine. 


So the two files corresponding to the Compments for vehciles by OEM 1 are read in and the rows in column "ID_Motor" which start with K1BE1 are filtered and only the two relevant colonm ID_Motor and ID Fahrzeug are selected

```{r, results='markup'}

#read files of OEM 1
OEM1_11 <- fread(file = "./Data/Fahrzeug/Bestandteile_Fahrzeuge_OEM1_Typ11.csv",header= T) %>%
    as_tibble() %>%
    filter(str_detect(ID_Motor,"^K1BE1")) %>%
    dplyr::select(ID_Motor,ID_Fahrzeug) %>%
    rename(ID_Engine=ID_Motor) %>%
    rename(ID_Vehicle=ID_Fahrzeug)


OEM1_12 <- fread(file = "./Data/Fahrzeug/Bestandteile_Fahrzeuge_OEM1_Typ12.csv",header= T) %>%
  as_tibble() %>%
  filter(str_detect(ID_Motor,"^K1BE1")) %>%
  dplyr::select(ID_Motor,ID_Fahrzeug) %>%
  rename(ID_Engine=ID_Motor) %>%
  rename(ID_Vehicle=ID_Fahrzeug)

#bind rows together of the two dataframes to cover all vehicles of OEM1
vehicles <- bind_rows(OEM1_11,OEM1_12)

#join components list with the vehicles of OEM 2 by ID Engine
vehicles_part4 <- components_part4 %>%
  left_join(vehicles, by=c("ID_K1BE1"="ID_Engine"))%>%
  rename(ID_Engine=ID_K1BE1)

head(vehicles_part4)
```

Next the registration table is read and tidied.

```{r, results='markup'}
#read and tidy registration file 
registration <-fread(file="./Data/Zulassungen/Zulassungen_alle_Fahrzeuge.csv",header = T) %>%
    as_tibble() %>%
    mutate(Zulassung = as.Date(Zulassung)) %>%
    rename(registration_date=Zulassung) %>%
    rename(IDNumber=IDNummer)%>%
    rename(municipalities=Gemeinden)

head(registration)
```

To find the vehicles which are registered in Dortmund the registration dataset is filtered and semi joined with the vehicles dataset to find only the matching Vehicle ID's. Finally the nrow() function gives us the answer that 19609 part 4 ended up in vehicles  registered in Dortmund.

```{r, results='markup'}
vehicles_dortmund <- registration %>%
  filter(str_detect(municipalities,"DORTMUND")) %>%
  semi_join(vehicles_part4, by=c("IDNumber"="ID_Vehicle"))
    
head(vehicles_dortmund)
nrow(vehicles_dortmund)
```

### 4.*Which data types do the attributes of the registration table "Zulassungen_aller_Fahrzeuge" have? Put your answers into a table which is integrated into your Markdown document*


The data types of the attributes can be determined by calling the str() function. This shows that 
1. V1 is integer
2. IDNumber is character
3. municipalities is character
4. registration_date is Date

```{r, results='markup'}
print(str(registration))
```


### 5.*You want to publish your application. Why does it make sense to store the data sets in a database on a server? Why is it not recommended to store the data sets on your personal computer? Name at least three points per question.*

It makes sense to store the data sets, which are used for the application, in a database on a server, because this makes it possible for other users to access the data sets and thus use the application properly. Additionaly databases can handle very large datasets better than the personal computer. 
Last but not least, databases are fault-tolerant. 

It is not recommended to store the data sets on your personal computer, because, unlike data stored in a database, data stored on the personal computer does not have a built-in structure. This makes it espacially for large datasets hard to structure . Furthermore datasets store on the personal computer are fault tolerant which can create fatal damage to the functionality of the application.

...

### 6. *On 11 August 2019 there was an accident involving a stolen car produced by your company. The driver left the scene without a trace. The license plate of the car, which caused the accident, was faked and the Vehicle Identification Number (VIN) was removed. Since you work for the Federal Motor Transport Authority, the police asks for your help to find out where the vehicle with the engine code "K1DI2-103-1031-21" (corresponds to the engine ID number) was registered.*

Through the shortcuts table by the chair we know that the beginning K1DI2 of the Engine ID belongs to OEM 2. Therefor we only read in the two compoments table for the two types of OEM 2. Next we combine the two tables by the function bindrows() and only select the relevant columns. Next the column "ID_Engine" of the data frame total_vehicles is filtered for the Engine ID "K1DI2-103-1031-21" and the registration data frame is joined by the Vehicle ID. This operation is then saved as driver. Since the final dataset contains 1111 rows and different registration muncipalities it is not possible to determine the vehicle, which caused the accident correctly. 

```{r, results='markup'}
#read compoments table for OEM 2 type 21
OEM2_21 <- fread(file = "./Data/Fahrzeug/Bestandteile_Fahrzeuge_OEM2_Typ21.csv",header= T) %>%
  as_tibble()

#read compoments table for OEM 2 type 212
OEM2_22 <- fread(file = "./Data/Fahrzeug/Bestandteile_Fahrzeuge_OEM2_Typ22.csv",header= T) %>%
  as_tibble()

#bind the rows of the files together
total_vehicles <- bind_rows(OEM2_21,OEM2_22) %>%
  rename(ID_Engine=ID_Motor) %>%
  rename(ID_Vehicle=ID_Fahrzeug) %>%
  dplyr::select(ID_Engine,ID_Vehicle)

#find vehicle with Engine ID and join dataset with registration table by the Vehicle ID
driver <- total_vehicles%>%
  filter(str_detect(ID_Engine,"K1DI2-103-1031-21")) %>%
  left_join(registration, by= c("ID_Vehicle"="IDNumber"))

head(driver)
```

